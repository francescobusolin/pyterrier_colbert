{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pyterrier as pt\n",
    "import ujson\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "from colbert.modeling.inference import ModelInference\n",
    "from colbert.evaluation.loaders import load_colbert\n",
    "from pyterrier_colbert import load_checkpoint\n",
    "# monkeypatch to use our downloading version\n",
    "import colbert.evaluation.loaders\n",
    "\n",
    "colbert.evaluation.loaders.load_checkpoint = load_checkpoint\n",
    "colbert.evaluation.loaders.load_model.__globals__['load_checkpoint'] = load_checkpoint\n",
    "from colbert.utils.utils import print_message\n",
    "import pickle\n",
    "from colbert.indexing.index_manager import IndexManager\n",
    "from warnings import warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier_colbert.preprocessing import DatasetPreprocessor, TokenRemover, HFTokenizer, NLTKTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=\"http://www.dcs.gla.ac.uk/~craigm/colbert.dnn.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-barcelona",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Object()\n",
    "args.similarity = 'cosine'\n",
    "args.dim = 128\n",
    "args.query_maxlen = 32\n",
    "args.doc_maxlen = 180\n",
    "args.checkpoint = checkpoint\n",
    "args.mask_punctuation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpiece = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "hf_tokenizer = HFTokenizer(tokenizer=wordpiece)\n",
    "nltk_tokenizer = NLTKTokenizer(tokenizer_type='treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier_colbert.ranking import ColBERTFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyterrier_colbert_factory = ColBERTFactory(checkpoint, \"./indexes/\", \"index.base.vaswani\",memtype='mmap')\n",
    "colbert_e2e = pyterrier_colbert_factory.end_to_end()\n",
    "retrievers['base'] = colbert_e2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner_names = ['en', 'en2' ,'en4', 'few',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in cleaner_names:\n",
    "    factory = ColBERTFactory(checkpoint, f'./indexes', f'index.clean.{name}.vaswani')\n",
    "    rete2e = factory.end_to_end()\n",
    "    retrievers[name] = rete2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pt.get_dataset(\"vaswani\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.Experiment(\n",
    "    list(retrievers.values()),\n",
    "    dataset.get_topics(),\n",
    "    dataset.get_qrels(),\n",
    "    eval_metrics=[\"recip_rank\", \"ndcg_cut_10\", 'mrt'],\n",
    "    names = list(retrievers.keys()),\n",
    "    baseline=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_remover = TokenRemover(tokenizer=nltk_tokenizer, stopwords_files=['stopwords/stopwords-en.txt'])\n",
    "en2_remover = TokenRemover(tokenizer=nltk_tokenizer, stopwords_files=['stopwords/stopwords-en.txt'], stopword_max_length=2)\n",
    "en4_remover = TokenRemover(tokenizer=nltk_tokenizer, stopwords_files=['stopwords/stopwords-en.txt'], stopword_max_length=4)\n",
    "en_punc_remover = TokenRemover(tokenizer=nltk_tokenizer, stopwords_files=['stopwords/stopwords-en.txt', 'stopwords/stopwords-punctuations.txt'])\n",
    "lim_remover = TokenRemover(tokenizer=nltk_tokenizer, stopwords_files=['stopwords/stopwords-limited.txt'])\n",
    "lim_punc_remover = TokenRemover(tokenizer=nltk_tokenizer, stopwords_files=['stopwords/stopwords-limited.txt', 'stopwords/stopwords-punctuations.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "en2_remover.stopwords = en2_remover.stopwords | set(['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_remover.stopwords), len(en2_remover.stopwords),len(en4_remover.stopwords) ,len(en_punc_remover.stopwords), len(lim_remover.stopwords), len(lim_punc_remover.stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaners = [('en', en_remover),('en2', en2_remover),('en4', en4_remover) ,('few', lim_remover)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_cleaned = {name: DatasetPreprocessor(dataset=dataset, tokenizer=nltk_tokenizer, preprocessor=cleaner) for name, cleaner in cleaners}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set()\n",
    "for file in ['stopwords/stopwords-limited.txt']:\n",
    "    with open(file, 'r') as f:\n",
    "        stopwords = stopwords | set(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tokenizer, stopwords, maxl, x):\n",
    "    text = tokenizer.tokenize(x)\n",
    "    tokens = [tok for tok in text if tok not in stopwords]\n",
    "    return tokenizer.detokenize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_topics = dataset.get_topics().copy()\n",
    "clean_topics['query'] = clean_topics['query'].map(lambda x: clean(nltk_tokenizer, stopwords, 512 ,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.Experiment(\n",
    "    list(retrievers.values()),\n",
    "    clean_topics,\n",
    "    dataset.get_qrels(),\n",
    "    eval_metrics=[\"recip_rank\", \"ndcg_cut_10\", 'mrt'],\n",
    "    names = list(retrievers.keys()),\n",
    "    baseline=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-assistant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cleanbert)",
   "language": "python",
   "name": "cleanbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
