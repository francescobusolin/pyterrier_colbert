{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pyterrier as pt\n",
    "import ujson\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "from colbert.modeling.inference import ModelInference\n",
    "from colbert.evaluation.loaders import load_colbert\n",
    "from pyterrier_colbert import load_checkpoint\n",
    "# monkeypatch to use our downloading version\n",
    "import colbert.evaluation.loaders\n",
    "\n",
    "colbert.evaluation.loaders.load_checkpoint = load_checkpoint\n",
    "colbert.evaluation.loaders.load_model.__globals__['load_checkpoint'] = load_checkpoint\n",
    "from colbert.utils.utils import print_message\n",
    "import pickle\n",
    "from colbert.indexing.index_manager import IndexManager\n",
    "from warnings import warn\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from colbert.modeling import colbert as CBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "racial-wrestling",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unlike-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier_colbert.preprocessing import DatasetPreprocessor, TokenRemover, HFTokenizer, NLTKTokenizer, DoNothingPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "julian-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "immune-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_colbert(args):\n",
    "    print_message(\"#> Loading model checkpoint.\")\n",
    "    colbert = CBERT.ColBERT.from_pretrained('bert-base-uncased',\n",
    "                                      query_maxlen=args.query_maxlen,\n",
    "                                      doc_maxlen=args.doc_maxlen,\n",
    "                                      dim=args.dim,\n",
    "                                      similarity_metric=args.similarity, mask_punctuation=args.mask_punctuation)\n",
    "    DEVICE = 'cuda:0' if faiss.get_num_gpus() > 0 else 'cpu'\n",
    "    colbert = colbert.to(DEVICE)\n",
    "    checkpoint = load_checkpoint(args.checkpoint, colbert)\n",
    "    colbert.eval()\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    return colbert, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "supposed-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=\"http://www.dcs.gla.ac.uk/~craigm/colbert.dnn.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ruled-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Object()\n",
    "args.similarity = 'cosine'\n",
    "args.dim = 128\n",
    "args.query_maxlen = 32\n",
    "args.doc_maxlen = 180\n",
    "args.checkpoint = checkpoint\n",
    "args.mask_punctuation = False\n",
    "args.amp = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "lightweight-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pt.get_dataset('vaswani')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "european-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpiece = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "hf_tokenizer = HFTokenizer(tokenizer=wordpiece)\n",
    "nltk_tokenizer = NLTKTokenizer(tokenizer_type='treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "local-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned = DatasetPreprocessor(dataset=dataset, tokenizer=hf_tokenizer, preprocessor=DoNothingPreprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "least-transmission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 05, 09:55:31] #> Loading model checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ColBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 05, 09:55:32] #> Loading checkpoint http://www.dcs.gla.ac.uk/~craigm/colbert.dnn.zip\n",
      "[Mar 05, 09:55:39] #> checkpoint['epoch'] = 0\n",
      "[Mar 05, 09:55:39] #> checkpoint['batch'] = 44500\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "colbert, model_checkpoint = load_colbert(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "outside-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colbert.modeling.inference import ModelInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "favorite-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = ModelInference(colbert=colbert, amp=args.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "legitimate-round",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 128])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference.docFromText(['I am happy']).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "derived-hebrew",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.5830], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = wordpiece(['What is the ultimate answer to life the universe and everything?'])\n",
    "d = wordpiece(['The is the is to!'])\n",
    "cq = [ torch.tensor(q['input_ids']), torch.tensor(q['attention_mask']) ]\n",
    "cd = [ torch.tensor(d['input_ids']), torch.tensor(d['attention_mask']) ]\n",
    "              \n",
    "colbert(Q=cq, D=cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "seventh-atlas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'the', 'ultimate', 'answer', 'to', 'everything', '?']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece.tokenize('What is the ultimate answer to everything?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developmental-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "supposed-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast, AutoTokenizer, \\\n",
    "    AutoModelForSequenceClassification, RobertaForSequenceClassification, ElectraForSequenceClassification, \\\n",
    "    RobertaTokenizerFast, ElectraTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "clear-minority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7005, 0.9147, 0.0208], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"veneres/monobert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dict_tokenizer = tokenizer(\n",
    "    [\"What is the ultimate answer to life the universe and everything?\", \n",
    "     \"What is the ultimate answer to life the universe and everything?\", \n",
    "     \"What is the ultimate answer to life the universe and everything?\" ],\n",
    "    [\n",
    "       \"The ultimate answer is 42.\", \"ultimate answer 42\",\"The is the is to!\"\n",
    "    ],\n",
    "    return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model(**dict_tokenizer)\n",
    "\n",
    "print(torch.softmax(model(**dict_tokenizer).logits, dim=1)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-familiar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-doctor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cleanbert)",
   "language": "python",
   "name": "cleanbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
